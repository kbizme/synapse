# <img src="logo.png" align="center" width="32" alt="synapse"> Synapse

Synapse is a **minimal agentic GenAI chat application** built to demonstrate practical understanding of **LangChain**, **LLM agents**, and **RAG (Retrieval-Augmented Generation)**, served via **FastAPI** with a lightweight vanilla JS frontend.

The project intentionally prioritizes **clarity, modularity, and explainability** over production-scale complexity.

## ‚ú® Key Features

- ü§ñ **Agentic LLM Chat**
  - Built using LangChain‚Äôs `create_agent`
  - Powered by Groq-hosted foundation models. Eg. `llama-3.1-8b-instant`, `openai/gpt-oss-20b`
  - Supports short-term conversational memory within a chat

- üìÑ **RAG (Retrieval-Augmented Generation)**
  - Upload **PDF or TXT** documents
  - Agent can retrieve relevant context from uploaded documents
  - Works seamlessly alongside normal LLM chat

- üß† **Conversational Memory**
  - Agent remembers recent messages in the same chat
  - Memory is scoped to a single user multi-chat session (with persistence as well)

- üß© **Modular Architecture**
  - Clean separation between API layer, Agent logic, Tools, and Orchestration
  - Designed to be easily extended (auth, multi-user, LangGraph)

- üåê **Minimal Frontend**
  - Vanilla HTML, CSS, and JavaScript
  - Simple chat interface for demonstration and testing

## üéØ Purpose of This Project

Synapse exists to demonstrate:

- Agent-based GenAI application design
- Practical LangChain usage
- Clean backend architecture
- Thoughtful engineering tradeoffs

It is intentionally minimal, modular, and extensible.

## üèóÔ∏è Architecture Overview

``` mermaid
graph TD
    %% Node Definitions with Bold Titles
    A["<b>FRONTEND (UI)</b><br/>HTML + CSS + Vanilla JS<br/>‚Ä¢ Chat input<br/>‚Ä¢ File upload (PDF/TXT)<br/>‚Ä¢ Message rendering"]
    
    B["<b>FASTAPI BACKEND</b><br/>‚Ä¢ Request validation<br/>‚Ä¢ Routing<br/>‚Ä¢ Lifecycle management"]
    
    C["<b>LANGCHAIN RUNTIME</b><br/>‚Ä¢ Agent executor<br/>‚Ä¢ Memory<br/>‚Ä¢ Tools (<b>RAG</b>)"]
    
    D["<b>GROQ Foundation Models</b><br/> ‚Ä¢ LlaMa models<br/> ‚Ä¢ OpenAI models <br/> + Vector Store (<b>RAG</b>)"]

    %% Flow Connections with bold labels
    A ==>|<b>&nbsp; HTTP JSON / Multipart &nbsp;</b>| B
    B ==> C
    C ==> D

    %% Styling for High Visibility
    %% Using high-contrast borders and semi-transparent fills 
    %% that work in both Light and Dark modes
    style A fill:#3498db22,stroke:#3498db,stroke-width:3px,rx:10,ry:10
    style B fill:#9b59b622,stroke:#9b59b6,stroke-width:3px,rx:10,ry:10
    style C fill:#2ecc7122,stroke:#2ecc71,stroke-width:3px,rx:10,ry:10
    style D fill:#e67e2222,stroke:#e67e22,stroke-width:3px,rx:10,ry:10

    %% Edge styling
    linkStyle default stroke:#888,stroke-width:2px
```

## üß† How RAG Works in Synapse

1. User uploads a PDF or TXT file
2. Document is loaded and split into chunks
3. Chunks are embedded and stored in an in-memory vector store
4. RAG is exposed to the agent as a **tool**
5. During chat:
   - Agent decides whether to call the RAG tool
   - Retrieved context is injected into the reasoning process
   - Final answer is generated by the LLM

If no document is uploaded, the agent behaves like a normal conversational LLM.

## üíæ Memory Model

- Short-term, thread-safe in-process memory
- Maintains recent conversation turns
- Scoped to single user multi chat management simultaneously
- Persistence across restarts

This design keeps behavior predictable while remaining extensible to:

- LangGraph checkpoints
- Multi-user isolation (future scope)

## üö´ Out of Scope (Intentional)

- Authentication & authorization
- Multi-user support
- Advanced UI frameworks (Like. React or Angular)

These are intentionally excluded to keep the project focused and explainable.

## üîÆ Future Extensions

- Multi-user support with authentication
- System Controlled Adaptive RAG Implementation
- LangGraph-based orchestration
- Replacing Streaming responses using SSE (Server Sent Events)
- Tool expansion (web search, sandboxed code execution, etc.)

## ‚ñ∂Ô∏è Running the Application (Local)

1. Clone the repository
2. Create a virtual environment: `virtualenv .pyenv`
3. Activate the virtual environment:
    - On Windows: `.pyenv/scripts/activate`
    - On Linux/Mac: `source .pyenv/bin/activate`
4. Install dependencies: `pip install -r requirements.txt`
5. Create a `.env` file in the root project folder.
   - Add Groq API key to `.env` as `GROQ_API_KEY`=`...`
   - Optionally, You can add these LangSmith Tracing parameters as well:

      - `LANGCHAIN_TRACING_V2`=`true`
      - `LANGCHAIN_ENDPOINT`=`https://api.smith.langchain.com`
      - `LANGCHAIN_API_KEY`=`...`
      - `LANGCHAIN_PROJECT`=`synapse`
6. Start the FastAPI server
    - Open a Terminal and run this command: `uvicorn app.main:app --reload`
7. Open another Terminal and start the Frontend server using Python http:
   - Go to `frontend` folder: `cd frontend`
   - Run `python -m http.server 5500`

8. Open the frontend in a browser and navigate to: `http://127.0.0.1:5500`
9. OR You can test the FastAPI endspoints on: `http://127.0.0.1:8000/docs`

---

## üñºÔ∏è Example Screenshots

- Frontend UI Pages of the Synapse Application
  
  ![Home Page](ui-screenshots/screenshot_0.png)

- Chat Screenshots
  
  ![Chat UI Screenshot](ui-screenshots/screenshot_1.png)
  
  ![Chat UI Screenshot](ui-screenshots/screenshot_2.png)

- FastAPI Endpoints

  ![Chat UI Screenshot](ui-screenshots/screenshot_3.png)